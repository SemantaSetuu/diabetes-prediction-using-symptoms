# app.py ‚Äì Early‚ÄëStage Diabetes Risk Predictor (all‚Äëin‚Äëone)
# ---------------------------------------------------------
# ‚Ä¢ Embeds build_preprocessor and registers a fake `data_processing` module
#   so joblib can un‚Äëpickle the saved LightGBM pipeline.
# ‚Ä¢ Presents a Streamlit UI for inference.
# ---------------------------------------------------------

# ----------------- std‚Äëlib + third‚Äëparty imports -----------------
import sys, types
from pathlib import Path

import pandas as pd
import joblib
import streamlit as st
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from lightgbm.sklearn import LGBMClassifier  # noqa: F401  (needed for un‚Äëpickle)
from sklearn.preprocessing import LabelEncoder

# ----------------- 1Ô∏è‚É£  clone of build_preprocessor -----------------
TARGET_COLUMN = "class"

def build_preprocessor(df: pd.DataFrame):
    """Return a ColumnTransformer that one‚Äëhot‚Äëencodes categorical cols."""
    cat_cols, num_cols = [], []
    for col in df.columns:
        if col == TARGET_COLUMN:
            continue
        if df[col].dtype == "object":
            cat_cols.append(col)
        else:
            num_cols.append(col)

    cat_pipe = Pipeline([("onehot", OneHotEncoder(handle_unknown="ignore"))])

    return ColumnTransformer(
        [("categorical", cat_pipe, cat_cols),
         ("numeric", "passthrough", num_cols)]
    )

# ----------------- 2Ô∏è‚É£  register fake module for unpickling ----------
fake_mod = types.ModuleType("data_processing")
fake_mod.build_preprocessor = build_preprocessor
sys.modules["data_processing"] = fake_mod     # üéØ un‚Äëpicker will find it here

# ----------------- 3Ô∏è‚É£  Streamlit page config -----------------------
st.set_page_config(page_title="Diabetes Risk Predictor", page_icon="ü©∫")

# ----------------- 4Ô∏è‚É£  load model (cached) -------------------------
MODEL_PATH = "lightgbm_symptom_full.pkl"

@st.cache_resource(show_spinner="Loading ML model‚Ä¶")
def load_model(path: str):
    if not Path(path).exists():
        st.error(f"Model file not found: {path}")
        st.stop()
    return joblib.load(path)

model = load_model(MODEL_PATH)
st.success("‚úÖ Model ready for prediction!")

# ----------------- 5Ô∏è‚É£  UI ‚Äì collect user inputs --------------------
st.title("ü©∫ Early‚ÄëStage Diabetes Prediction")

age    = st.slider("Age (years)", 1, 120, 40)
gender = st.radio("Gender", ["Male", "Female"])

symptom_questions = {
    "Polyuria¬†(excessive urination)": "Polyuria",
    "Polydipsia¬†(excessive thirst)": "Polydipsia",
    "Sudden weight loss": "sudden weight loss",
    "Weakness": "weakness",
    "Polyphagia¬†(excessive hunger)": "Polyphagia",
    "Genital thrush": "Genital thrush",
    "Visual blurring": "visual blurring",
    "Itching": "Itching",
    "Irritability": "Irritability",
    "Delayed healing": "delayed healing",
    "Partial paresis": "partial paresis",
    "Muscle stiffness": "muscle stiffness",
    "Alopecia": "Alopecia",
    "Obesity": "Obesity",
}

st.markdown("### Symptom checklist")
user_symptoms = {
    col: st.radio(q, ["No", "Yes"], key=col)
    for q, col in symptom_questions.items()
}

# ----------------- 6Ô∏è‚É£  assemble input & predict --------------------
def make_input():
    record = {"Age": age, "Gender": gender}
    record.update(user_symptoms)
    return pd.DataFrame([record])

if st.button("üîÆ¬†Predict"):
    X = make_input()
    try:
        pred       = model.predict(X)[0]          # 0 / 1
        pred_prob  = model.predict_proba(X)[0][1] # probability of Positive
    except Exception as e:
        st.error(f"Prediction failed: {e}")
        st.stop()

    result = "Positive¬†(1)" if pred == 1 else "Negative¬†(0)"
    st.subheader(f"**Prediction:** {result}")
    st.write(f"Probability of being Positive: **{pred_prob:.3f}**")

    with st.expander("See model input"):
        st.dataframe(X)

st.caption(
    "‚ÑπÔ∏è¬†This app is for educational purposes only. "
    "Consult a qualified healthcare professional for medical advice."
)
